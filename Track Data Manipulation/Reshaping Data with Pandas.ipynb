{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecdb050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cbb4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A) Wide Format: Each feature is in a seperate column\n",
    "# #       Each row contain many features of the same player\n",
    "# #       no repetition but large number of missing values\n",
    "# #       preferred for simple statistics\n",
    "# #       Unit of Analysis: each player\n",
    "# # B) Long Format: Each row represent one feature.\n",
    "# #       Multiple row for each player, one for each feature\n",
    "# #       Columns are mostly: Name, Variable, Value\n",
    "# #       for Tidy Data: Better to summarize, Key-Value pair, preferred for analysis and graphing\n",
    "# #       Unit of Analysis: a certain characteristic of a player\n",
    "\n",
    "# # Elementry Functions: Transpose, shape, and drop\n",
    "# df = pd.read_csv('Cities1.csv')\n",
    "# df.shape\n",
    "# df.set_index('row')[['pop','city']].transpose()\n",
    "# df.drop(4, axis=0)   # deleting the 5th row  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a35455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshaping using pivots (for converting long tables to wide tables)\n",
    "# # generally : .pivot(index = 'name', columns = 'variable')\n",
    "# df1 = pd.read_csv('Sample1.csv') \n",
    "# df1r = df.pivot(index='city',columns='year', values='pop')\n",
    "# print(df1)\n",
    "# print(df1r)\n",
    "\n",
    "# df2 = pd.read_csv('Sample2.csv') \n",
    "# df2r = df2.pivot(index='city',columns='variable', values='year70')\n",
    "# df2r2 = df2.pivot(index='city',columns='variable', values=['year70','year80'])\n",
    "\n",
    "# print(df2)\n",
    "# print(df2r2)\n",
    "\n",
    "# # The pivot table is sensitive to repeated values and reply error. \n",
    "# # We should delete one of them by .drop() syntax.\n",
    "# # We can leave values, which makes the pivot to have all values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adadb0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Limitations of Pivot Tables:\n",
    "# # Pivots cannot aggregate values. / Index/column pair should be unique. \n",
    "# # Thus, here we will use pivot_table instead of pivot\n",
    "\n",
    "# df3 = pd.read_csv('Sample3.csv') \n",
    "# # df3r = df3.pivot(index='city',columns='variable', values='year70')      # This line will have error\n",
    "# df3r = df3.pivot_table(index = 'city', columns = 'variable', values='year70', aggfunc= 'mean')\n",
    "# df3r = df3.pivot_table(index = 'city', columns = 'variable', values=['year70','year80'], aggfunc= 'mean')\n",
    "# df3r = df3.pivot_table(index = 'city', columns = 'variable', values=['year70','year80'], aggfunc= 'sum', margins=True)\n",
    "# print(df3)\n",
    "# print(df3r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1e106c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Wide to Long Transformation: \n",
    "# # A) with melt: \n",
    "# df1 = pd.read_csv('Cities1.csv')\n",
    "# m1df1 = df1.melt(id_vars=['row','city'])\n",
    "# m2df1 = df1.melt(id_vars=['row'])\n",
    "# m3df1 = df1.melt(id_vars=['row'],value_vars='pop')\n",
    "# m4df1 = df1.melt(id_vars=['row'],value_vars='pop', var_name='feature', value_name='amount')\n",
    "\n",
    "# print(df1)\n",
    "# print(m1df1)\n",
    "# print(m4df1)\n",
    "\n",
    "# # B) Wide to Long Function\n",
    "# df = pd.read_csv('Sample4.csv')\n",
    "# # ldf1 = pd.wide_to_long(df, stubnames=['pop','gdp'], i = 'city' , j = 'year')        # This line will face error because of repetitive rows\n",
    "# df1 = df.drop(2,axis=0)\n",
    "# ldf1 = pd.wide_to_long(df1, stubnames=['pop','gdp'], i = 'city' , j = 'year')\n",
    "# # ldf1 = pd.wide_to_long(df1, stubnames=['pop','gdp'], i = 'city' , j = 'year', sep='_') # In the case we had pop_60 instead of pop60\n",
    "# # ldf1 = pd.wide_to_long(df1, stubnames=['pop','gdp'], i = 'city' , j = 'year', sep='_', suffix = '\\w+') \n",
    "# # # In the case we had pop_one, pop_two instead of pop_60, pop_70\n",
    "# print(df1)\n",
    "# print(ldf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# # Set title and authors as unique indexes. Extract the prefixes isbn and prefix. Name the new variable version.\n",
    "# all_long = pd.wide_to_long(golden_age, \n",
    "#                            stubnames=['isbn', 'prefix'], \n",
    "#                            i=['title', 'authors'], \n",
    "#                            j='version')\n",
    "\n",
    "\n",
    "# # Example 2\n",
    "# # Modify the books_hunger DataFrame by resetting the index without dropping it.\n",
    "# # Reshape books_hunger from wide to long format. Use the columns title and language as unique indexes. \n",
    "# # Name feature the new variable created from the columns that starts with publication and page. \n",
    "# # Those columns are separated by a blank space and end in a word.\n",
    "\n",
    "# # Modify books_hunger by resetting the index without dropping it\n",
    "# books_hunger.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# # Reshape using title and language as index, feature as new name, publication and page as prefix separated by space and ending in a word\n",
    "# publication_features = pd.wide_to_long(books_hunger, \n",
    "#                                        stubnames=['publication', 'page'], \n",
    "#                                        i=['title', 'language'], \n",
    "#                                        j='feature', \n",
    "#                                        sep=' ', \n",
    "#                                        suffix='\\w+')\n",
    "\n",
    "# # Print publication_features\n",
    "# print(publication_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b364f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Working with Strings\n",
    "# # 1: Seperating\n",
    "\n",
    "# df = pd.read_csv('Sample5.csv')\n",
    "# st1 = df['city'].str.split(\":\")\n",
    "# st2 = df['city'].str.split(\":\").str.get(1)\n",
    "# st3 = df['city'].str.split(\":\", expand = True)      # This one is a dataframe\n",
    "# df[['main title', 'subtitle']] = st3\n",
    "# df = df.drop('city', axis=1)\n",
    "# df1 = pd.wide_to_long(df,stubnames=['pop','gdp'], i = ['main title', 'subtitle'] , j = 'year')\n",
    "# print(df1)\n",
    "\n",
    "# # 2: Concating\n",
    "\n",
    "# df = pd.read_csv('Sample6.csv')\n",
    "# df['complete_form'] = df['city'].str.cat(df['country'], sep = '::')\n",
    "# df1 = df.melt(id_vars='complete_form', value_vars=['pop60','pop70'], var_name='feature', value_name = 'value')\n",
    "# print(df1)\n",
    "\n",
    "# nums = ['one','two','three','four','five','six','seven','eight']\n",
    "# df['city'] = df['city'].str.cat(nums,\"-\")\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e9f0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# # Split by the hyphen the index of books_dys\n",
    "# books_dys.index = books_dys.index.str.split('-').str.get(0)\n",
    "\n",
    "# # Concatenate the index with the list author_list separated by a hyphen\n",
    "# books_dys.index = books_dys.index.str.cat(author_list, sep='-')\n",
    "# print(books_dys)\n",
    "\n",
    "\n",
    "# # Example 2\n",
    "# # Define a DataFrame hp_melt by melting the goodreads and amazon columns into a single column named source. \n",
    "# # Assign the name rating to the resulting value column. \n",
    "# # Use only the full title and the writer as identifier variables.\n",
    "\n",
    "# # Concatenate the title and subtitle separated by \"and\" surrounded by spaces\n",
    "# hp_books['full_title'] = hp_books['title'].str.cat(hp_books['subtitle'], sep =\" and \") \n",
    "\n",
    "# # Split the authors into writer and illustrator columns\n",
    "# hp_books[['writer', 'illustrator']] = hp_books['authors'].str.split('/', expand=True)\n",
    "\n",
    "# # Melt goodreads and amazon columns into a single column \n",
    "# hp_melt = hp_books.melt(id_vars=['full_title', 'writer'], \n",
    "#                         var_name='source', \n",
    "#                         value_vars=['goodreads', 'amazon'], \n",
    "#                         value_name='rating')\n",
    "# print(hp_melt)\n",
    "\n",
    "\n",
    "# # Example 3\n",
    "# # Split main_title by a colon and assign it to two columns named title and subtitle \n",
    "# books_sh[['title', 'subtitle']] = books_sh['main_title'].str.split(':', expand=True)\n",
    "\n",
    "# # Split version by a space and assign the second element to the column named volume \n",
    "# books_sh['volume'] = books_sh['version'].str.split(' ').str.get(1)\n",
    "\n",
    "# # Drop the main_title and version columns modifying books_sh\n",
    "# books_sh.drop(['main_title', 'version'], axis=1, inplace=True)\n",
    "\n",
    "# # Reshape using title, subtitle and volume as index, name feature the new variable from columns starting with number, separated by undescore and ending in words\n",
    "# sh_long = pd.wide_to_long(books_sh, stubnames='number', i=['title', 'subtitle', 'volume'], \n",
    "#                           j='feature', sep='_', suffix='\\w+')\n",
    "\n",
    "# # Print sh_long\n",
    "# print(sh_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bc6014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multi-Index\n",
    "# # Setting multi indexes, method 1: first two columns are converted to indexes\n",
    "# city = pd.read_csv('Cities2.csv')\n",
    "# city.set_index(['row','city'], inplace=True)\n",
    "\n",
    "# # method 2: adding two new indexes\n",
    "# city = pd.read_csv('Cities2.csv')\n",
    "# new_array = [['yes','yes','no','no','yes','yes','no'],['no','yes','yes','yes','no','yes','no']]\n",
    "# city2 = city.copy()\n",
    "# city2.index = pd.MultiIndex.from_arrays(new_array, names=['row','city'])\n",
    "# print(city)\n",
    "\n",
    "# # Another exammple (which the code does not work), in which we can design indexes and column names manually\n",
    "# # Creating multi-level indexes on the rows and columns\n",
    "\n",
    "# index = pd.MultiIndex.from_arrays([['Wick', 'Wick', 'Jack', 'Kack'],['Abasi', 'Bani', 'Caty', 'Davudi']],\n",
    "#                                   names = ['last','first'])\n",
    "# columns = pd.MultiIndex.from_arrays([['2019','2019','2020','2020'],['age','weight','age','weight']],\n",
    "#                                     names = ['year','feature'])\n",
    "# recreacted_df = pd.DataFrame(data,index=index,columns=columns) \n",
    "\n",
    "# # Stack method: \n",
    "# # rearrange a level of columns to obtain a reshaped DF with a new inner most level row index\n",
    "# city3 = city2.stack()\n",
    "# print(city3)\n",
    "# # we can set the level\n",
    "# city4 = city2.stack(level=0)   # we can set a column_name too. \n",
    "# print(city4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fb73a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# # New week, new project! One of your clients, a telecommunication company, wants to know why its customers are leaving. \n",
    "# # You will perform an analysis to figure it out. First, you explored the dataset churn and realized some information is missing. \n",
    "# # The dataset contains data about the total number of calls and the minutes spent on the phone by different customers. \n",
    "# # However, the state and city they live in are not listed.\n",
    "# # You predefined an array with that data. You'd like to add it as an index in your DataFrame.\n",
    "# # The DataFrame churn is available for you. \n",
    "# # It contains data about area code, total_day_calls and total_day_minutes. Make sure to examine it in the console!\n",
    "\n",
    "# # Predefined list to use as index\n",
    "# new_index = [['California', 'California', 'New York', 'Ohio'], \n",
    "#              ['Los Angeles', 'San Francisco', 'New York', 'Cleveland']]\n",
    "\n",
    "# # Create a multi-level index using predefined new_index\n",
    "# churn_new = pd.MultiIndex.from_arrays(new_index, names=['state', 'city'])\n",
    "\n",
    "# # Assign the new index to the churn index\n",
    "# churn.index = churn_new\n",
    "\n",
    "# # Reshape by stacking churn DataFrame\n",
    "# churn_stack = churn.stack()\n",
    "# print(churn_stack)\n",
    "\n",
    "# # Example 2\n",
    "# # Set state and city as index modifying the DataFrame\n",
    "# churn.set_index(['state', 'city'], inplace=True)\n",
    "\n",
    "# # Reshape by stacking the second level\n",
    "# churn_stack = churn.stack(level=1)\n",
    "# print(churn_stack)\n",
    "\n",
    "\n",
    "# # Example 3\n",
    "# # Stack churn by the feature column level\n",
    "# churn_feature = churn.stack(level='feature')\n",
    "# print(churn_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1531cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unstacking\n",
    "# info = pd.read_excel('stack.xlsx'); info.set_index(['Last','First'], inplace=True)\n",
    "# info_stacked = info.stack();  print(info_stacked)\n",
    "# info_unstacked1 = info_stacked.unstack(); print(info_unstacked1)\n",
    "# # info_unstacked2 = info_stacked.unstack(level=0); print(info_unstacked2) # we can set the level by number\n",
    "# # # above code make a column amount a new column list\n",
    "# info_unstacked3 = info_stacked.unstack(level='Last'); print(info_unstacked3) # we can set the level by name\n",
    "# info_unstacked1.sort_index(ascending=False)\n",
    "\n",
    "# # We can also reaarange levels by combining a stack and unstack command. (the following example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e946499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# # This time, you'll reorganize a row index as a column index. After that, you will move a column index to a row index. \n",
    "# # To do this, you will first unstack the DataFrame, and then stack it.\n",
    "# # The same churn DataFrame is available for you. It contains data about minutes, calls, and charge for different times of the day, \n",
    "# # types of calls, and exited status. Make sure to examine it in the console!\n",
    "\n",
    "# # Unstack churn by type level\n",
    "# churn_type = churn.unstack(level='type')\n",
    "\n",
    "# # Stack churn_final using the first column level\n",
    "# churn_final = churn_type.stack(level=0)\n",
    "# print(churn_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49cba5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rearranging multiple levels\n",
    "# info = pd.read_excel('stack.xlsx'); info.set_index(['First','Last'], inplace=True); print(info)\n",
    "# info1 = info.swaplevel(1,0);  print(info1)           \n",
    "# info2 = info.swaplevel(1,0).unstack();  print(info1)           \n",
    "# # The above code would work well if we had repeated variable names in the 'First' column instead of unique names.\n",
    "\n",
    "# # we can unstack several levels as follows:\n",
    "# info3 = info.unstack(level=[0,1]); print(info3); print(info3)\n",
    "# info4 = info.unstack(level=['First','Last']); print(info4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# # Switch the first and third row index levels in churn\n",
    "# churn_swap = churn.swaplevel(0, 2)\n",
    "\n",
    "# # Reshape by unstacking the last row level \n",
    "# churn_unstack = churn_swap.unstack()\n",
    "# print(churn_unstack)\n",
    "\n",
    "# # Example 2\n",
    "# # Unstack the first and second row level of churn\n",
    "# churn_unstack = churn.unstack(level=[0, 1])\n",
    "\n",
    "# # Stack the resulting DataFrame using plan and year\n",
    "# churn_py = churn_unstack.stack(['plan', 'year'])\n",
    "\n",
    "# # Switch the first and second column levels\n",
    "# churn_switch = churn_py.swaplevel(0, 1, axis=1)\n",
    "# print(churn_switch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "688ca04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Handling missing data\n",
    "# # Unstacking can lead us to have missing values\n",
    "# info1 = info.unstack(level='First', fill_value='No');   print(info1)\n",
    "# # By defaults, the stack code remove rows with only missing values\n",
    "# # We can instead:\n",
    "# info.stack(dropna=False).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bc32ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# # You finished reshaping your churn dataset in the previous exercises. \n",
    "# # Now, it is ready to be used. You remember that something caught your attention. You are sure you saw a clear pattern in the data.\n",
    "# # Before you fit a classification model, you decide to do something simpler. \n",
    "# # You want to see what else you can learn from the data. You will reshape your data by unstacking levels, \n",
    "# # but you know this process will generate missing data that you need to handle.\n",
    "# # The churn DataFrame contains different features of customers located in Los Angeles and New York, and is available for you. \n",
    "# # Make sure to examine it in the console!\n",
    "\n",
    "# # Unstack churn level and fill missing values with zero\n",
    "# churn = churn.unstack(level='churn', fill_value=0)\n",
    "\n",
    "# # Sort by descending voice mail plan and ascending international plan\n",
    "# churn_sorted = churn.sort_index(level=[\"voice_mail_plan\", \"international_plan\"], \n",
    "#                                 ascending=[False, True])\n",
    "# print(churn_sorted)\n",
    "\n",
    "\n",
    "# # Example 2\n",
    "# # Stack the level scope without dropping rows with missing values\n",
    "# churn_stack = churn.stack(level='scope', dropna=False)\n",
    "\n",
    "# # Fill the resulting missing values with zero \n",
    "# churn_fill = churn_stack.fillna(0)\n",
    "# print(churn_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74d04d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combining Data. The code does not work\n",
    "# # Total amount of online and on-site sales:\n",
    "# sales.stack().sum(axis = 1).unstack()\n",
    "\n",
    "# # Mean amount of product sales by year in both countries\n",
    "# sales.unstack(level = 0).mean(axis = 1)\n",
    "\n",
    "# # Difference in amount sales between sales\n",
    "# sales[\"office supply\"].unstack(level = 'country').diff(axis = 1, periods = 2)\n",
    "\n",
    "# # Total amount of different products by online or on-site regardless of the country\n",
    "# sales.stack().groupby(level = 'shop').sum()\n",
    "\n",
    "# # Median amount of products by year\n",
    "# sales.groupby(level = 'year').median()\n",
    "# # we can reshape the result by:\n",
    "# sales.groupby(level = 'year').median().stack(level = [0,1]).unstack(level = 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea9c01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# # Unstack the first level and calculate the mean of the columns\n",
    "# obesity_general = obesity.unstack(level=0).mean(axis = 1)\n",
    "\n",
    "# # Unstack the second level and calculate the mean of the columns\n",
    "# obesity_mean = obesity.unstack(level = 1).mean(axis = 1)\n",
    "\n",
    "# # Unstack the third level and calculate the difference between columns\n",
    "# obesity_variation = obesity.unstack(level=2).diff(axis=1)\n",
    "\n",
    "# # Stack obesity, get median of columns and unstack again\n",
    "# median_obesity = obesity.stack().median(axis = 1).unstack()\n",
    "\n",
    "# # Stack the first level, get sum, and unstack the second level\n",
    "# obesity_sum = obesity.stack(level = 0).sum(axis = 1).unstack(level = 1)\n",
    "\n",
    "# # Stack country level, group by country and get the mean \n",
    "# obesity_mean = obesity.stack(level='country').groupby('country').mean()\n",
    "\n",
    "# # Stack country level, group by country and get the median \n",
    "# obesity_median = obesity.stack(level='country').groupby('country').median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f093b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transfrom a list-like column (100,1345,23) to seperate rows. \n",
    "# cities = pd.read_excel('cities_zip.xlsx')\n",
    "# cities_exploded = cities['zip_code'].explode()\n",
    "# cities[['city','country']].merge(cities_exploded, left_index = True, right_index = True)\n",
    "\n",
    "# # Faster method:\n",
    "# cities_exploded2 = cities.explode('zip_code')\n",
    "# # reseting the index:\n",
    "# cities_exploded2.reset_index(drop=True, inplace=True)  # a new index from 0 to n-1 will be created\n",
    "# cities_exploded2\n",
    "\n",
    "# # If the codes above does not work, the zip code is considered a string, which can be treated like:\n",
    "# cities = pd.read_excel('cities_zip.xlsx')\n",
    "# cities.assign(zip_code = cities['zip_code'].str.split(',')).explode('zip_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82289dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# # Explode the values of bounds to a separate row\n",
    "# obesity_bounds = obesity['bounds'].explode()\n",
    "\n",
    "# # Merge obesity_bounds with country and perc_obesity columns of obesity using the indexes\n",
    "# obesity_final = obesity[['country', 'perc_obesity']].merge(obesity_bounds, \n",
    "#                                         right_index=True, \n",
    "#                                         left_index=True)\n",
    "# print(obesity_final)\n",
    "\n",
    "# # Example 2\n",
    "# # Transform the list-like column named bounds  \n",
    "# obesity_explode = obesity.explode('bounds')\n",
    "\n",
    "# # Modify obesity_explode by resetting the index \n",
    "# obesity_explode.reset_index(drop=True, inplace=True)\n",
    "# print(obesity_explode)\n",
    "\n",
    "# # Example 3\n",
    "# # Transform the column bounds in the obesity DataFrame\n",
    "# obesity_split = obesity.assign(bounds=obesity['bounds'].str.split('-')).explode('bounds')\n",
    "# print(obesity_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # JSON format: easily used by both human and machines. The codes does not work\n",
    "# # very similar structures to dictionaries\n",
    "# # Nested Json:\n",
    "# # Dictionary in Dictionary or list structures\n",
    "# # Consider the following json structure:\n",
    "# # writers = [\n",
    "# #             {\n",
    "# #                 \"first\" : \"Mary\"\n",
    "# #                 \"Last\" : \"Shelby\"\n",
    "# #                 \"books\" : {\"title\" : \"a\", 'year': 1818}\n",
    "# #             },\n",
    "# #             {\n",
    "# #                 \"first\" : \"Ernest\"\n",
    "# #                 \"Last\" : \"Hemingway\"\n",
    "# #                 \"books\" : {\"title\" : \"b\", 'year': 1919}\n",
    "# #             }\n",
    "            \n",
    "# #         ]\n",
    "# from pandas import json_normalize\n",
    "# writers_pd = json_normalize(writers)     # makes a dataframe from a json\n",
    "# writers_pd2 = json_normalize(writers, sep = '-')\n",
    "# # reshaping:\n",
    "# pd.wide_to_long(writes_pd2, stubnames=['books'], i = ['first', 'last'], j = 'feature', sep = '-', suffix='\\w+')\n",
    "\n",
    "# # In the following JSON format consider that the books is itself a list of book with different titles and years\n",
    "# json_normalize(writers, record_path = 'books')      # Create a dataframe consists off only books parameters\n",
    "# json_normalize(writers, record_path = 'books', meta = ['name', 'last']) # The name and last columns will be added to the end of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# # You are curious about a movies dataset you've had on your computer for some time now that contains data about different movies. \n",
    "# # You would like to analyze that data, but you realize it's in a nested JSON format.\n",
    "# # To read it into a DataFrame, you will need to use the function you have just learned. \n",
    "# # After that, you will reshape the resulting DataFrame to make it easier to work with.\n",
    "# # The semi-structured JSON named movies is available for you.\n",
    "\n",
    "# # Normalize movies and separate the new columns with an underscore\n",
    "# movies_norm = json_normalize(movies, sep='_')\n",
    "\n",
    "# # Reshape using director and producer as index, create movies from column starting from features\n",
    "# movies_long = pd.wide_to_long(movies_norm, stubnames='features', \n",
    "#                               i=['director', 'producer'], j='movies', \n",
    "#                               sep='_', suffix='\\w+')\n",
    "# print(movies_long)\n",
    "\n",
    "# # Example 2\n",
    "# # Specify director and producer to use as metadata for each record \n",
    "# normalize_movies = json_normalize(movies, \n",
    "#                                   record_path='features', \n",
    "#                                   meta=['director','producer'])\n",
    "# print(normalize_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b70e28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # JSON Format - part 2\n",
    "# # Dealing with nested data columns\n",
    "# writers = [\"Mary Shelby\", \"Ernest Hamingway\"]\n",
    "# books = [{'title' : 'Frankestain', 'year': '1818'},\n",
    "#          {'title' : 'felan' , 'year' : '1951'}]\n",
    "# collection = pd.DataFrame(dict(writers = writers, books = books))\n",
    "# print(collection)\n",
    "\n",
    "# import json\n",
    "# books = collection['books'].apply(json.loads).apply(pd.Series) # This line does not work.\n",
    "# collection = collection.drop(columns='books')\n",
    "# pd.concat([collection,books], axis=1)\n",
    "# # The result is a new DF with seperated books column\n",
    "\n",
    "# # Another approach:\n",
    "# books = collection['books'].apply(json.load).to_list()\n",
    "# book_dump = json.dumps(books)\n",
    "# new_books = pd.read_json(book_dump)\n",
    "# pd.concat([collection['writers'], new_books], axis=1)\n",
    "# # The result will be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5c97700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# # Define birds reading names and bird_facts lists into names and bird_facts columns\n",
    "# birds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))\n",
    "\n",
    "# # Apply to bird_facts column the function loads from json module\n",
    "# data_split = birds['bird_facts'].apply(json.loads).apply(pd.Series)\n",
    "\n",
    "# # Remove the bird_facts column from birds\n",
    "# birds = birds.drop(columns='bird_facts')\n",
    "\n",
    "# # Concatenate the columns of birds and data_split\n",
    "# birds = pd.concat([birds, data_split], axis=1)\n",
    "# print(birds)\n",
    "\n",
    "# # Example 2\n",
    "# # Apply json.loads to the bird_facts column and transform it to a list\n",
    "# birds_facts = birds['bird_facts'].apply(json.loads).to_list()\n",
    "\n",
    "# # Convert birds_fact into a JSON \n",
    "# birds_dump = json.dumps(birds_facts)\n",
    "\n",
    "# # Read the JSON birds_dump into a DataFrame \n",
    "# birds_df = pd.read_json(birds_dump)\n",
    "\n",
    "# # Concatenate the 'names' column of birds with birds_df \n",
    "# birds_final = pd.concat([birds['names'], birds_df], axis=1)\n",
    "# print(birds_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
