{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecdb050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import h5py\n",
    "import datetime as dt\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d93b41dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Removing string from numbers (M, B, ...) + Assertion\n",
    "# df = pd.read_csv('number_string.csv')\n",
    "# df['pop'] = df['pop'].str.strip('M')\n",
    "# df['pop'] = df['pop'].astype('int')\n",
    "# # verifying\n",
    "# assert df['pop'].dtype == 'int'\n",
    "# df['category'] = df['category'].astype('category')\n",
    "# df.describe()   # the category column is removed\n",
    "# df['category'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5aabfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deal with Out-of-range Data\n",
    "# # Approches: 1) drop 2) setting custom min and max for columns 3) treat as missing data (later)\n",
    "# # 1: Dropping:\n",
    "# movies = pd.read_csv('movies.csv')\n",
    "# movies2 = movies[movies['rating'] <=5]                              # Solution 1\n",
    "# movies1 = movies.copy()\n",
    "# movies1.drop(movies1[movies1['rating']>5].index, inplace = True)       # Solution 2\n",
    "# assert movies1['rating'].max() <= 5\n",
    "\n",
    "# # 2: Setting range:\n",
    "# movies3 = movies.copy()\n",
    "# movies3.loc[movies3['rating']>5, 'rating'] = 5 \n",
    "# assert movies3['rating'].max() <= 5\n",
    "\n",
    "# # Special case: dates\n",
    "# # for dates to be comparable, we first need to change their format\n",
    "# user_signup['subs_date'] = pd.to_datetime(user_signup['sub_date']).dt.date\n",
    "# today_date = dt.date.today()\n",
    "# user_signup = user_signup[user_signup['subs_date']<today_date]\n",
    "# # or\n",
    "# user_signup.drop(user_signup[user_signup['subs_date']>today_date].index, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dbc14ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# # Convert tire_sizes to integer\n",
    "# ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "# ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "\n",
    "# # Reconvert tire_sizes back to categorical\n",
    "# ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "# print(ride_sharing['tire_sizes'].describe())\n",
    "\n",
    "# # Example 2\n",
    "# # Convert ride_date to date\n",
    "# ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
    "# today = dt.date.today()\n",
    "# ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "# print(ride_sharing['ride_dt'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73a94efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uniqeness Condition, dealing with duplicate values\n",
    "# column_name = ['film']      # It can be several column names\n",
    "# duplicates = movies.duplicated(subset=column_name, keep=False)\n",
    "# # keep: first (keep first), last, False (keep all)\n",
    "# movies[duplicates].sort_values(by = 'film')\n",
    "# movies4 = movies.copy()\n",
    "# movies4.drop_duplicates(subset = column_name, inplace=True, keep='last')\n",
    "# # keep works the same\n",
    "# # inplace = True: drop duplicated rows directly inside dataframe without creating new obj.\n",
    "\n",
    "# # more sophisticated method:\n",
    "# summaries = {'rating':'max', 'sales':'mean'}\n",
    "# movies5 = movies.groupby(by = column_name).agg(summaries).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e7fd8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# duplicates = ride_sharing.duplicated(subset = 'ride_id', keep = False)\n",
    "# duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "# print(duplicated_rides[['ride_id','duration','user_birth_year']])\n",
    "\n",
    "# # Example 2\n",
    "# # Drop complete duplicates from ride_sharing\n",
    "# ride_dup = ride_sharing.drop_duplicates()\n",
    "# statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "# ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
    "\n",
    "# # Find duplicated values again\n",
    "# duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "# duplicated_rides = ride_unique[duplicates == True]\n",
    "# assert duplicated_rides.shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b50775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Membership constraints, working with categories\n",
    "# # Solutions when we face a problem:\n",
    "# # 1) dropping   2) remapping    3) inferring them\n",
    "\n",
    "# blood = pd.read_csv('blood.csv')\n",
    "# categories = ['A+', 'B+', 'AB+', 'O+', 'A-', 'B-', 'AB-', 'O-']\n",
    "# catgry = pd.DataFrame(categories, columns=['category'])\n",
    "# print(blood['group'].unique())\n",
    "# inconsistents = set(blood['group']).difference(catgry['category'])\n",
    "# inconsistent_rows = blood['group'].isin(inconsistents)\n",
    "# blood[~inconsistent_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49cfc42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# # Print categories DataFrame\n",
    "# print(categories.info())\n",
    "\n",
    "# # Print unique values of survey columns in airlines\n",
    "# print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "# print('Safety: ', airlines['Safety'].unique(), \"\\n\")\n",
    "# print('Satisfaction: ', airlines['Satisfaction'].unique(), \"\\n\")\n",
    "\n",
    "# # Find the cleanliness category in airlines not in categories\n",
    "# cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# # Find rows with that category\n",
    "# cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# # Print rows with inconsistent category\n",
    "# print(airlines[cat_clean_rows])\n",
    "\n",
    "# # Print rows with consistent categories only\n",
    "# print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b3edeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Working with Categories - continue\n",
    "\n",
    "# # Value Inconsistency, \n",
    "# # upper lower strings case + spaces\n",
    "# marrge = pd.read_csv('Marriage.csv')\n",
    "# marrge_status = marrge['marrge_st']\n",
    "# count = marrge_status.value_counts()\n",
    "# print(count)\n",
    "# marrge['marrge_st'] = marrge['marrge_st'].str.strip()   # remove spaces\n",
    "# marrge['marrge_st'] = marrge['marrge_st'].str.upper()\n",
    "# count = marrge_status.value_counts()\n",
    "# print(count)\n",
    "\n",
    "# # creating category based on money\n",
    "# group_name = ['A', 'B', 'C']\n",
    "# marrge['wealth_ctgry'] = pd.qcut(marrge['money'], q=3, labels=group_name)\n",
    "# marrge[['money','wealth_ctgry']]\n",
    "# # instead:\n",
    "# ranges = [0,10,40,np.inf]\n",
    "# marrge['wealth_ctgry'] = pd.cut(marrge['money'], bins=ranges, labels=group_name)\n",
    "# marrge[['money','wealth_ctgry']]\n",
    "\n",
    "# # Collapse Catehories\n",
    "# A = [['A','BBB'],['B','BB'],['C','BBBB'],['D','AA'],['E','AAAA']]\n",
    "# A_d=pd.DataFrame(A, columns=['name','value'])\n",
    "# mapping = {'BBB':'B', 'BB':'B', 'BBBB':'B', 'BBc':'B', 'AA':'A', 'AAAA':'A'}\n",
    "# A_d['value'] = A_d['value'].replace(mapping)\n",
    "# print(A_d['value'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b813c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# # Print unique values of both columns\n",
    "# print(airlines['dest_region'].unique())\n",
    "# print(airlines['dest_size'].unique())\n",
    "\n",
    "# # Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "# airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
    "# airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# # Remove white spaces from `dest_size`\n",
    "# airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# # Verify changes have been effected\n",
    "# print(airlines['dest_size'].unique())\n",
    "# print(airlines['dest_region'].unique())\n",
    "\n",
    "# # Example 2\n",
    "# label_ranges = [0, 60, 180, np.inf]\n",
    "# label_names = ['short', 'medium', 'long']\n",
    "# airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "#                                 labels = label_names)\n",
    "\n",
    "# mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "#             'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "#             'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "# airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "28339e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleaning Text Data\n",
    "# # Working and preparing phone numbers texts\n",
    "# phones['phone number'] = phones['phone number'].str.replace(\"+\",\"00\")\n",
    "# phones['phone number'] = phones['phone number'].str.replace(\"-\",\"\")\n",
    "# digits = phones['phone number'].str.len()\n",
    "# phones.loc[digits < 10, 'phone number'] = np.nan\n",
    "# sanity_check = phones['phone number'].str.len()\n",
    "# assert sanity_check.min() >= 10\n",
    "# # check numbers do not have \"+\" or \"-\"\n",
    "# assert phones['phone number'].str.contains(\"+|-\").any() ==  False\n",
    "\n",
    "# # regular expressions \n",
    "# phones['phone number'] = phones['phone number'].str.replace(r\"\\D+\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b35505a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
    "# airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
    "# airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\",\"\")\n",
    "# airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\",\"\")\n",
    "# assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False\n",
    "\n",
    "# # Example 2\n",
    "# resp_length = airlines['survey_response'].str.len()\n",
    "# airlines_survey = airlines[resp_length > 40]\n",
    "# assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "# print(airlines_survey['survey_response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cae8976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uniformity\n",
    "# # First we can plot a scatter to find outliers\n",
    "# plt.scatter(x = 'Date', y = 'temperature', data = Temperatures)\n",
    "# # For example, by plotting the temperatures, we find out that three points are too high and their unit should be ferenhite.\n",
    "# temp_frnh = Temperatures.loc[Temperatures['temperature']>40,'temperature']\n",
    "# temp_celc = (temp_frnh - 32) / (5/9)\n",
    "# Temperatures.loc[Temperatures['temperature']>40,'temperature'] = temp_celc\n",
    "# assert Temperatures['temperature'].max() < 40\n",
    "\n",
    "# # Different Types of Birthday Formats\n",
    "# # most dates are recognizable by datetime format (pd.to_datetime())\n",
    "# births['birthday'] = pd.to_datetime(births['birthday'])   # This way we will face errors (ValueError) probably\n",
    "# births['birthday'] = pd.to_datetime(births['birthday'],\n",
    "#                                     # Attempt to infer the format of each date\n",
    "#                                     infer_date_time_format = True,\n",
    "#                                     # Return NA where conversion failed\n",
    "#                                     errors = 'coerce')\n",
    "\n",
    "# # for setting a specific format\n",
    "# births['birthday'] = births['birthday'].dt.strftime(\"%d-%m-%y\")\n",
    "\n",
    "# # Generally, it is essential to take into account the source of data before starting to clean it  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "81e0fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# acct_eu = banking['acct_cur'] == 'euro'\n",
    "# banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "# banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "# assert banking['acct_cur'].unique() == 'dollar'\n",
    "\n",
    "# # Example 2\n",
    "# # Print the header of account_opend\n",
    "# print(banking['account_opened'].head())\n",
    "\n",
    "# # Convert account_opened to datetime\n",
    "# banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "#                                            # Infer datetime format\n",
    "#                                            infer_datetime_format = True,\n",
    "#                                            # Return missing value for error\n",
    "#                                            errors = 'coerce') \n",
    "\n",
    "# # Get year of account opened\n",
    "# banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# # Print acct_year\n",
    "# print(banking['acct_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b6371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cross field validation: using multiple fields in a detaset to sanity check the integrity of them\n",
    "# integ = pd.read_csv('Integrity.csv')\n",
    "# sum_classes = integ[['seatA','seatB','seatC']].sum(axis=1)\n",
    "# check_eq = sum_classes == integ['Total']\n",
    "# inconsistent_seats = integ[~check_eq]\n",
    "# consistent_seats = integ[check_eq]\n",
    "\n",
    "# # Another example: The code does not work\n",
    "# users['birthday'] = pd.to_datetime(users['birthday'])\n",
    "# today = dt.date.today()\n",
    "# age_manual = today.year - users['birthday'].dt.year\n",
    "# age_eq = age_manual == users['Age']\n",
    "# inconsistent_age = users[~age_eq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0903920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example \n",
    "# fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "# inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']\n",
    "# consistent_inv = banking[inv_equ]\n",
    "# inconsistent_inv = banking[~inv_equ]\n",
    "# print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3ce9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Completeness, dealing with missing data\n",
    "# # Missing Types: \n",
    "# # Missing completely @ random: no systematic relationship between missing data and other values\n",
    "# # Missing random \n",
    "# # Missing not @ random:There is a systematic relationship between missing data and observed ones\n",
    "# # Approaches to deal with them:\n",
    "# # Drop / impute with statiscal measures like mean, median, ... / \n",
    "# # impute an algorithm approach / machine learning approach\n",
    "\n",
    "# information = pd.read_csv('missing.csv')\n",
    "# print(information.isna().sum())        # .sum() provides a summary\n",
    "# missing = information['pop'].isna() | information['money'].isna()\n",
    "# missed_data = information[missing]\n",
    "# complete_data = information[~missing]\n",
    "# complete_data.describe()\n",
    "\n",
    "# info2 = information.dropna(subset=['pop'])\n",
    "# # OR\n",
    "# pop_mean = information['pop'].mean()\n",
    "# info3 = information.fillna({'pop':pop_mean})\n",
    "\n",
    "\n",
    "# # for visualizing the missing data:\n",
    "# import missingno as msno\n",
    "# msno.matrix(information)\n",
    "# plt.show()\n",
    "\n",
    "# sorted_pop = information.sort_values(by = 'pop', ascending=True)\n",
    "# msno.matrix(sorted_pop)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15282b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# print(banking.isna().sum())\n",
    "# msno.matrix(banking)\n",
    "# plt.show()\n",
    "# missing_investors = banking[banking['inv_amount'].isna()]\n",
    "# investors = banking[~banking['inv_amount'].isna()]\n",
    "# banking_sorted = banking.sort_values(by = 'age')\n",
    "# msno.matrix(banking_sorted)\n",
    "# plt.show()\n",
    "\n",
    "# # Example 2\n",
    "# # Drop missing values of cust_id\n",
    "# banking_fullid = banking.dropna(subset = ['cust_id'])\n",
    "\n",
    "# # Compute estimated acct_amount\n",
    "# acct_imp = banking_fullid['inv_amount'] * 5\n",
    "\n",
    "# # Impute missing acct_amount with corresponding acct_imp\n",
    "# banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
    "\n",
    "# # Print number of missing values\n",
    "# print(banking_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1310bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Minimum editing distance: a systematic way to realize how close two strings are.\n",
    "# # The # of operation(insert, delete, change the position, ...) needed to convert a string to another.\n",
    "# # There are several packages for doing so such as fuzz\n",
    "# from thefuzz import fuzz\n",
    "# from thefuzz import process\n",
    "# print(fuzz.WRatio('Reeding','Reading'))        # 0: not similar   100: the same\n",
    "# print(fuzz.WRatio('Houston Rockets','Rockets'))\n",
    "# print(fuzz.WRatio(\"Houston Rockets vs Los Angeles Lakers\", 'Rockets vs Lakers'))\n",
    "# string1 = \"Houston Rockets vs Los Angeles Lakers\"\n",
    "# choices = pd.Series(['Rockets vs Lakers','Lakers vs Rockets','Houson vs Los Angeles','Heat vs Bulls'])\n",
    "# print(process.extract(string1, choices, limit = choices.shape[0]))    # For comparing a string with an array of strings\n",
    "\n",
    "# # Now we want to categorize a list based on the similarity of its elements\n",
    "# categories = pd.DataFrame(['New York', 'California'], columns=['state'])\n",
    "# survey = pd.read_csv('state.csv')\n",
    "# for state in categories['state']:\n",
    "#     matches = process.extract(state, survey['state'], limit=survey.shape[0])\n",
    "#     for potential_matches in matches:\n",
    "#         if potential_matches[1] >= 80:\n",
    "#             survey.loc[survey['state'] == potential_matches[0], 'state'] = state\n",
    "\n",
    "# print(survey)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a39ba1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 1\n",
    "# from thefuzz import process\n",
    "# unique_types = restaurants['cuisine_type'].unique()\n",
    "\n",
    "# # Calculate similarity of 'asian', 'american', and 'italian' to all values of unique_types\n",
    "# print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
    "# print(process.extract('american', unique_types, limit = len(unique_types)))\n",
    "# print(process.extract('italian', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# # Example 2\n",
    "# # Iterate through categories\n",
    "# for cuisine in categories:  \n",
    "#   # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "#   matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "#   # Iterate through the list of matches\n",
    "#   for match in matches:\n",
    "#      # Check whether the similarity score is greater than or equal to 80\n",
    "#     if match[1] >= 80:\n",
    "#       # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "#       restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "      \n",
    "# # Inspect the final result\n",
    "# print(restaurants['cuisine_type'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bf8c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Record Linkage. The code does not work\n",
    "# # Here we have two dataframes and we want to find pairs between them\n",
    "# # works well for data with different formats\n",
    "\n",
    "# import recordlinkage\n",
    "# indexer = recordlinkage.Index()\n",
    "# indexer.block('state')\n",
    "# pairs = indexer.index(census_A, census_B)\n",
    "\n",
    "# compare_cl = recordlinkage.Compare()\n",
    "\n",
    "# compare_cl.exact('date_of_birth','date_of_birth', label = 'date_of_birth')\n",
    "# compare_cl.exact('state','state', label = 'state')\n",
    "\n",
    "# compare_cl.string('surname','surname',threshold = 0.85, label = 'surname')\n",
    "# compare_cl.string('address_1','address_1',threshold = 0.85, label = 'address_1')\n",
    "\n",
    "# potential_matches = compare_cl.compute(pairs, census_A, census_B)\n",
    "# matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "092cdd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# # In the last lesson, you cleaned the restaurants dataset to make it ready for building a restaurants recommendation engine. \n",
    "# # You have a new DataFrame named restaurants_new with new restaurants to train your model on, that's been scraped from a new data source.\n",
    "# # You've already cleaned the cuisine_type and city columns using the techniques learned throughout the course. \n",
    "# # However you saw duplicates with typos in restaurants names that require record linkage instead of joins with restaurants.\n",
    "# # In this exercise, you will perform the first step in record linkage and generate possible pairs of \n",
    "# # rows between restaurants and restaurants_new. \n",
    "# # Both DataFrames, pandas and recordlinkage are in your environment.\n",
    "\n",
    "# # Create an indexer and object and find possible pairs\n",
    "# indexer = recordlinkage.Index()\n",
    "\n",
    "# # Block pairing on cuisine_type\n",
    "# indexer.block('cuisine_type')\n",
    "\n",
    "# # Generate pairs\n",
    "# pairs = indexer.index(restaurants, restaurants_new)\n",
    "\n",
    "# # In the last exercise, you generated pairs between restaurants and restaurants_new\n",
    "# # in an effort to cleanly merge both DataFrames using record linkage.\n",
    "# # When performing record linkage, there are different types of matching you can perform between different \n",
    "# # columns of your DataFrames, including exact matches, string similarities, and more.\n",
    "# # Now that your pairs have been generated and stored in pairs, you will find exact matches in the city and \n",
    "# # cuisine_type columns between each pair, and similar strings for each pair in the rest_name column. \n",
    "# # Both DataFrames, pandas and recordlinkage are in your environment.\n",
    "\n",
    "# # Create a comparison object\n",
    "# comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# # Find exact matches on city, cuisine_types - \n",
    "# comp_cl.exact('city', 'city', label='city')\n",
    "# comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
    "\n",
    "# # Find similar matches of rest_name\n",
    "# comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) \n",
    "\n",
    "# # Get potential matches and print\n",
    "# potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "# print(potential_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cccdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The rest of previous block:\n",
    "# duplicate_rows = matches.index.get_level_values(1)\n",
    "# census_B_dupl = census_B[census_B.index.isin(duplicate_rows)]\n",
    "# census_B_new = census_B[~census_B.index.isin(duplicate_rows)]\n",
    "# full_census = census_A.append(census_B_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example - continued\n",
    "# # In the last lesson, you've finished the bulk of the work on your effort to link restaurants and restaurants_new. \n",
    "# # You've generated the different pairs of potentially matching rows, searched for exact matches between the cuisine_type and city columns, \n",
    "# # but compared for similar strings in the rest_name column. You stored the DataFrame containing the scores in potential_matches.\n",
    "# # Now it's finally time to link both DataFrames. \n",
    "# # You will do so by first extracting all row indices of restaurants_new that are matching across the \n",
    "# # columns mentioned above from potential_matches. \n",
    "# # Then you will subset restaurants_new on these indices, then append the non-duplicate values to restaurants. \n",
    "# # All DataFrames are in your environment, alongside pandas imported as pd.\n",
    "\n",
    "# # Isolate potential matches with row sum >=3\n",
    "# matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
    "\n",
    "# # Get values of second column index of matches\n",
    "# matching_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# # Subset restaurants_new based on non-duplicate values\n",
    "# non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
    "\n",
    "# # Append non_dup to restaurants\n",
    "# full_restaurants = restaurants.append(non_dup)\n",
    "# print(full_restaurants)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
