{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ecdb050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import h5py\n",
    "import scipy.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e794ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Intro to Importing Data.ipynb'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Working with Directories\n",
    "import os\n",
    "wd = os.getcwd()\n",
    "os.listdir(wd)\n",
    "os.listdir(wd)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aa1cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Basic Functions\n",
    "# filename = \"Sample.csv\"\n",
    "# file = open(filename, mode = 'r')   # mode = 'w' is for writnig\n",
    "# text = file.read()\n",
    "# file.close()\n",
    "# print(file.closed)      # Check whether file is closed\n",
    "\n",
    "# # Another model:\n",
    "# with open('Sample.txt', 'r') as file1:\n",
    "#     print(file1.readline())\n",
    "#     print(file1.readline())\n",
    "    \n",
    "    \n",
    "# # This mode does not need closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61a85311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.recarray'>\n"
     ]
    }
   ],
   "source": [
    "# # Importing Data with Numpy\n",
    "# filename = 'numbers.txt'\n",
    "# data1 = np.loadtxt(filename,delimiter=',')      # Tab delimiter: '\\t'\n",
    "# data2 = np.loadtxt(filename, delimiter=',', skiprows=1)  # skiprow will skip the first row\n",
    "# data3 = np.loadtxt(filename, delimiter=',', usecols=[0,2])  # just first and third columns\n",
    "# data4 = np.loadtxt(filename, delimiter=',', dtype=str)  # convert data into strings\n",
    "\n",
    "# # Accessing rows and columns of structured arrays is super-intuitive: \n",
    "# # to get the ith row, merely execute data[i] and to get the column with name 'Fare', execute data['Fare'].\n",
    "\n",
    "# data5 = np.recfromcsv('numbers.csv')\n",
    "# print(type(data5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8980b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example, Reshape function\n",
    "# file = 'digits.csv'\n",
    "# digits = np.loadtxt(file, delimiter=',')\n",
    "# print(type(digits))\n",
    "# im = digits[21, 1:]\n",
    "# im_sq = np.reshape(im, (28, 28))\n",
    "# plt.imshow(im_sq, cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "# #Example 2\n",
    "# file = 'seaslug.txt'\n",
    "# data = np.loadtxt(file, delimiter='\\t', dtype=str)\n",
    "# print(np.shape(data))\n",
    "# data_float = np.loadtxt(file, delimiter='\\t', dtype='float', skiprows=1)\n",
    "\n",
    "# # Print the 10th element of data_float\n",
    "# print(data_float[9])\n",
    "\n",
    "# plt.scatter(data_float[:, 0], data_float[:, 1])\n",
    "# plt.xlabel('time (min.)');  plt.ylabel('percentage of larvae'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b848bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pandas, Advantage: work with different data types\n",
    "# # .value convert a dataframe type to numpy array\n",
    "# filename = 'Sample.csv'\n",
    "# data = pd.read_csv(filename, nrows=2)       # First two rows\n",
    "# data_array = data.values\n",
    "# # more complete version:\n",
    "# data = pd.read_csv(file, sep='\\t', comment='#', na_values=\"Nothing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29e20865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# file = 'titanic_corrupt.txt'\n",
    "# data = pd.read_csv(file, sep='\\t', comment='#', na_values=\"Nothing\")\n",
    "# pd.DataFrame.hist(data[['Age']])\n",
    "# plt.xlabel('Age (years)');  plt.ylabel('count');    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4831c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Other file Types\n",
    "# # Pickles are native to python. There are many datatypes that are not obvious how to store them.\n",
    "# # They are serialized, means converting objects to bytestream\n",
    "# import pickle\n",
    "# with open ('pickled_fruit.pkl', 'rb') as file:  # rb: read binary\n",
    "#     data = pickle.load(file)\n",
    "# print(data)\n",
    "\n",
    "# # Excel Files\n",
    "# file = 'numbers.xlsx'\n",
    "# data = pd.ExcelFile(file)\n",
    "# print(data.sheet_names)\n",
    "# df1 = data.parse('numbers')     # sheet name as a script\n",
    "# df2 = data.parse(0)             # sheet index as a int\n",
    "\n",
    "# # more complete form of parsing:\n",
    "# df2 = xls.parse(1, usecols=[0], skiprows=[0], names=['Country'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0626e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# # Parse the first sheet and rename the columns: df1\n",
    "# df1 = xls.parse(0, skiprows=[0], names=['Country','AAM due to War (2002)'])\n",
    "\n",
    "# # Parse the first column of the second sheet and rename the column: df2\n",
    "# df2 = xls.parse(1, usecols=[0], skiprows=[0], names=['Country'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3609bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing SAS, Stata files\n",
    "# # for SAS files\n",
    "\n",
    "# from sas7bdat import SAS7BDAT\n",
    "# with SAS7BDAT('file.sas7bdat') as file:\n",
    "#     df_sas = file.to_data_frame()\n",
    "    \n",
    "# # for Stata files:\n",
    "# data = pd.read_stata('file.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7dd54b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of SAS\n",
    "# with SAS7BDAT('sales.sas7bdat') as file:\n",
    "#     df_sas = file.to_data_frame()\n",
    "\n",
    "# pd.DataFrame.hist(df_sas[['P']])\n",
    "# plt.ylabel('count')\n",
    "# plt.show()\n",
    "\n",
    "# # Example of Stata\n",
    "# df = pd.read_stata('disarea.dta')\n",
    "\n",
    "# pd.DataFrame.hist(df[['disa10']])\n",
    "# plt.xlabel('Extent of disease')\n",
    "# plt.ylabel('Number of countries')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd0d2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing hdf5: Hierarchical Data Format 5\n",
    "# # for storing large data sets (hundreds of gigabytes)\n",
    "# import h5py\n",
    "# filename = 'file.hdf5'\n",
    "# data = h5py.File(filename,'r')\n",
    "# print(type(data))\n",
    "\n",
    "# for key in data.keys():\n",
    "#     print(key)              # Suppose the outputs are [meta, quality, strain]\n",
    "\n",
    "# print(type(data['meta']))\n",
    "\n",
    "# for key in data['meta'].keys():\n",
    "#     print(keys)             # suppose the outputs are [Description, Detection, Duration]\n",
    "\n",
    "# print(np.array(data['meta']['Description']), np.array(data['meta']['Detection']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example \n",
    "\n",
    "# file = 'LIGO_data.hdf5'\n",
    "# data = h5py.File(file, 'r')\n",
    "# for key in data.keys():\n",
    "#     print(key)\n",
    "\n",
    "# group = data['strain']\n",
    "\n",
    "# for key in group.keys():\n",
    "#     print(key)\n",
    "    \n",
    "# strain = np.array(data['strain']['Strain'])\n",
    "# num_samples = 10000\n",
    "\n",
    "# # Set time vector\n",
    "# time = np.arange(0, 1, 1/num_samples)\n",
    "\n",
    "# # Plot data\n",
    "# plt.plot(time, strain[:num_samples])\n",
    "# plt.xlabel('GPS Time (s)')\n",
    "# plt.ylabel('strain')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c05a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing Matlab files\n",
    "# # .mat files\n",
    "# import scipy.io\n",
    "# filename = 'file.mat'\n",
    "# mat = scipy.io.loadmat(filename)    \n",
    "# print(type(mat))                    # The mat is a dictionary\n",
    "# print(mat.keys())                   # Suppose the output is [x, y, z, ...]\n",
    "# print(type(mat['x']))               # The type can be a nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a5030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# mat = scipy.io.loadmat('albeck_gene_expression.mat')    \n",
    "# print(type(mat))\n",
    "# print(mat.keys())\n",
    "# print(type(mat['CYratioCyt']))\n",
    "# print(np.shape(mat['CYratioCyt']))\n",
    "\n",
    "# # Subset the array and plot it\n",
    "# data = mat['CYratioCyt'][25, 5:]\n",
    "# fig = plt.figure()\n",
    "# plt.plot(data)\n",
    "# plt.xlabel('time (min.)')\n",
    "# plt.ylabel('normalized fluorescence (measure of expression)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0dcaf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Relational Database Tables, where tables are linked \n",
    "# # There are 13 rules that a database should adhere to to be considered relational.\n",
    "# # Each row or record in a table represents an instance of an entity type.\n",
    "# # Each column in a table represents an attribute or feature of an instance.\n",
    "# # Every table contains a primary key column, which has a unique entry for each row.\n",
    "# # There are relations between tables.\n",
    "# # Most common systems: PostgredSQL, MySQL, and SQL (Structured Query Language)\n",
    "# # Now\n",
    "# # Crearing a database engine in python:\n",
    "# # SQLite database: Fast and simple\n",
    "# # SQLAlchemy: work with many relational database management systems\n",
    "# # The steps for SQL querying: 1)import packages and functions 2)create a database engine 3)connect to the engine\n",
    "# # 4)Query the database 5)save the result to a Dataframe  6)close the connection\n",
    "\n",
    "# from sqlalchemy import create_engine \n",
    "# engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "# table_names = engine.table_names()\n",
    "# print(table_names)\n",
    "\n",
    "# con = engine.connect()\n",
    "# rs = con.execute(\"SELECT * FROM Orders\")\n",
    "# df = pd.DataFrame(rs.fetchall())\n",
    "# df.columns = rs.keys()\n",
    "# con.close()\n",
    "\n",
    "# # Same Task with the Context Manager format:\n",
    "# from sqlalchemy import create_engine \n",
    "# engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "# with engine.connect() as con:\n",
    "#     rs = con.execute(\"SELECT OrderID, Shipname FROM Orders\")\n",
    "#     df = pd.DataFrame(rs.fetchmany(size = 5))   # This line will convert 5 rows instead of all rows\n",
    "#     df.columns = rs.keys()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36b79b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# # Here, you're going to fire up your very first SQL engine. \n",
    "# # You'll create an engine to connect to the SQLite database 'Chinook.sqlite', which is in your working directory. \n",
    "\n",
    "# from sqlalchemy import create_engine \n",
    "# engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "# table_names = engine.table_names()\n",
    "# print(table_names)\n",
    "\n",
    "# con = engine.connect()\n",
    "# rs = con.execute(\"SELECT * FROM Album\")\n",
    "# df = pd.DataFrame(rs.fetchall())\n",
    "# con.close()\n",
    "# print(df.head())\n",
    "\n",
    "# # Example 2\n",
    "# engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# # Open engine in context manager\n",
    "# with engine.connect() as con:\n",
    "#     rs = con.execute(\"SELECT * FROM Employee WHERE EmployeeId >= 6\")        # Setting some conditions\n",
    "#     # rs = con.execute(\"SELECT * FROM Employee ORDER BY BirthDate\")         # If we want to sort data by a specific column\n",
    "#     df = pd.DataFrame(rs.fetchall())\n",
    "#     df.columns = rs.keys()\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instead of codes above:\n",
    "# engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "# df = pd.read_sql_query('SELECT * FROM Orders', engine)\n",
    "# # more inputs:\n",
    "# # df = pd.read_sql_query('SELECT * FROM Employee WHERE EmployeeId >= 6 ORDER BY BirthDate', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Some Advanced Techniques with Relational Tables\n",
    "# # Inner Join:\n",
    "# engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "# df = pd.read_sql_query('SELECT Orders, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID', engine)\n",
    "# # It is very similar to inner join of pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c8a966c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example1\n",
    "# engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# # select all the records, extracting the Title of the record and Name of the artist of each record \n",
    "# # from the Album table and the Artist table, respectively. \n",
    "# # To do so, INNER JOIN these two tables on the ArtistID column of both.\n",
    "\n",
    "# with engine.connect() as con:\n",
    "#     rs = con.execute(\"SELECT Title, Name FROM Album INNER JOIN Artist on Album.ArtistID = Artist.ArtistID\")    \n",
    "#     df = pd.DataFrame(rs.fetchall())\n",
    "#     df.columns = rs.keys()\n",
    "    \n",
    "# print(df.head())\n",
    "\n",
    "# # Example 2\n",
    "# df = pd.read_sql_query(\n",
    "#     \"SELECT * FROM PlaylistTrack INNER JOIN Track ON PlaylistTrack.TrackId = Track.TrackId WHERE Milliseconds < 250000\",\n",
    "#     engine\n",
    "# )\n",
    "\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201a6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
