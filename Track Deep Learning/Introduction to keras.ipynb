{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecdb050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "885d3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A regression Deep Network Model\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# model = Sequential()\n",
    "# model.add(Dense(2,input_shape = (3,), activation = \"relu\"))\n",
    "# model.add(Dense(1))\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# model.summary()\n",
    "\n",
    "# # Fitting, prediction, and Evaluation\n",
    "# model.fit(X_train, Y_train, epochs=5)\n",
    "# pred = model.predict(X_test)\n",
    "# model.evaluate(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec138d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A Classifier\n",
    "# model = Sequential()\n",
    "# model.add(Dense(4,input_shape = (2,), activation = \"tanh\"))\n",
    "# model.add(Dense(1, activation = 'sigmoid'))\n",
    "# model.compile(optimizer='sgd', loss='binary_crossentripy')\n",
    "\n",
    "# model.train(coordinates, labels, epochs=20)\n",
    "# pred = model.predict(coordinates)\n",
    "# # model.evaluate(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e63c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Classifier. The code does not work\n",
    "# import seaborn as sns\n",
    "# sns.pairplot(banknotes, hue='class') \n",
    "# plt.show()\n",
    "# print('Dataset stats: \\n', banknotes.describe())\n",
    "# print('Observations per class: \\n', banknotes['class'].value_counts())\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(1, input_shape=(4,), activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "# model.fit(X_train, y_train, epochs = 20)\n",
    "# accuracy = model.evaluate(X_test, y_test)[1]\n",
    "# print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caabb126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multi_Class Classification\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# df = pd.read_csv('data.csv')\n",
    "# df.response = pd.Categorical(df.response)\n",
    "# df.response = df.response.cat.codes\n",
    "\n",
    "# # Creating one-hot encoding\n",
    "# y = to_categorical(df.response)\n",
    "\n",
    "# # Label Encoding\n",
    "# food_name       Category        calories\n",
    "# Apple           1               134\n",
    "# Orange          2               245\n",
    "# Banana          3               321\n",
    "# # One-hot Encoding\n",
    "# Apple       Orange      Banana      calories\n",
    "# 1           0           0           134\n",
    "# 0           1           0           245\n",
    "# 0           0           1           321\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of Multi-Class Classification\n",
    "# model = Sequential()\n",
    "# model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(4, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# darts.competitor = pd.Categorical(darts.competitor)\n",
    "# darts.competitor = darts.competitor.cat.codes \n",
    "# coordinates = darts.drop(['competitor'], axis=1)\n",
    "# competitors = to_categorical(darts.competitor)\n",
    "# model.fit(coord_train,competitors_train,epochs=200)\n",
    "# accuracy = model.evaluate(coord_test, competitors_test)[1]\n",
    "# print('Accuracy:', accuracy)\n",
    "# preds = model.predict(coords_small_test)\n",
    "\n",
    "# # Print preds vs true values\n",
    "# print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "# for i,pred in enumerate(preds):\n",
    "#   print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "# # Extract the position of highest probability from each pred vector\n",
    "# preds_chosen = [np.argmax(pred) for pred in preds]\n",
    "\n",
    "# # Print preds vs true values\n",
    "# print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
    "# for i,pred in enumerate(preds_chosen):\n",
    "#   print(\"{:25} | {}\".format(pred,competitors_small_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb4d681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multi-Label problem, in multi-class we have only one one, but in multi-label we can have several ones\n",
    "# model = Sequential()\n",
    "# model.add(Dense(2, input_shape = (1,)))\n",
    "# model.add(Dense(3, activation = 'sigmoid'))\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# model.fit(X_train, Y_train, epochs=100, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of a Multi-Label problem\n",
    "# model = Sequential()\n",
    "# model.add(Dense(64, input_shape=(20,), activation = 'relu'))\n",
    "# model.add(Dense(3, activation = 'sigmoid'))\n",
    "# model.compile(optimizer='adam', loss = \"binary_crossentropy\", metrics=['accuracy'])\n",
    "# model.summary()\n",
    "# model.fit(sensors_train, parcels_train, epochs = 100, validation_split = 0.2)\n",
    "# preds = model.predict(sensors_test)\n",
    "# preds_rounded = np.round(preds)\n",
    "# print('Rounded Predictions: \\n', preds_rounded)\n",
    "# accuracy = model.evaluate(sensors_test, parcels_test)[1]\n",
    "# print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Callbacks, Early Stopping, and model checkpoints\n",
    "# A = model.fit(X_train, y_train, epochs=100, metrics = ['accuracy'])\n",
    "# print(A.history['loss'])\n",
    "# print(A.history['accuracy'])\n",
    "# A = model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, y_test), metrics = ['accuracy'])\n",
    "# print(A.history['val_loss'])\n",
    "# print(A.history['val_accuracy'])\n",
    "\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# early_stopping = EarlyStopping(monitor = \"var_loss\", patience = 5)\n",
    "# A = model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# model_save = ModelCheckpoint('best_model.hdf5', save_best_only = True)\n",
    "# A = model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[model_save])\n",
    "# B = model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[model_save, early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de432f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of a plot for detecting Overfitting\n",
    "# h_callback = model.fit(X_train, y_train, epochs = 25, validation_data=(X_test, y_test))\n",
    "# plot_loss(h_callback.history[\"loss\"], h_callback.history[\"val_loss\"])\n",
    "# plot_accuracy(h_callback.history[\"accuracy\"], h_callback.history[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff76307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plotting Learning Curves, verbose\n",
    "# init_weights = model.get_weights()\n",
    "# train_accs = []\n",
    "# test_accs = []\n",
    "# for train_size in train_sizes:\n",
    "#     x_train_frac, _, y_train_frac, _ = train_test_split(x_train, y_train, train_size = train_size)\n",
    "#     model.set_weights(init_weights)\n",
    "#     model.fit(x_train_frac, y_train_frac, epochs=100, verbose=0, callbacks=[EarlyStopping(monitor = 'loss', patience=1)])\n",
    "#     train_acc = model.evaluate(x_train_frac, y_train_frac, verbose=0)[1]\n",
    "#     test_acc = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "#     train_accs.append(train_acc)\n",
    "#     test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Drawing a Learning Curves. The code does not work\n",
    "# h_callback = model.fit(X_train, y_train, epochs = 60, validation_data = (X_test, y_test), verbose=0)\n",
    "# plot_loss(h_callback.history[\"loss\"], h_callback.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: comparing activation functions. The code does not work\n",
    "# np.random.Seed(1)\n",
    "# def get_model(act_function):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(4, input_shpae = (3,), activation = act_function))\n",
    "#     model.add(Dense(1, activation = \"sigmoid\"))\n",
    "#     return model\n",
    "# activaions = [\"relu\", \"sigmoid\", \"tanh\"]\n",
    "# activaion_result = {}\n",
    "# for func in activaions:\n",
    "#     model = get_model(act_function = func)\n",
    "#     history = model.fit(X_train, y_train, epochs = 60, validation_data = (X_test, y_test), verbose=0)\n",
    "#     activaion_result[funct] = history\n",
    "# val_loss_per_func = {k:v.history[\"var_loss\"] for k,v in activaion_result.items()}\n",
    "# var_loss_curve = pd.DataFrame(val_loss_per_func)\n",
    "# val_loss_per_func.plot(title = \"Loss per activation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0440833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Batch size and batch normalization, which has several advantages\n",
    "# model.fit(X_train, y_train, epochs = 60, batch_size=128)\n",
    "# print(\"\\n The accuracy when using a batch of size 1 is: \", model.evaluate(X_test, y_test)[1])\n",
    "# from tensorflow.keras.layers import  BatchNormalization\n",
    "# model = Sequential()\n",
    "# model.add(Dense(4, input_shpae = (3,), activation = act_function))\n",
    "# # Batch normalization for the output of the layer above\n",
    "# model.add(BatchNormalization()) ## this code should be written after defining each layer\n",
    "# model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42cdaea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters, random search\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# def create_model(optimizer = \"adam\", activation = \"relu\"):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(5, input_shape=(4,), activation=activation))\n",
    "#     model.add(Dense(1, activation=\"sigmoid\"))\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "#     return model\n",
    "# model = KerasClassifier(build_fn=create_model, epochs= 6, batch_size = 16)\n",
    "# params = dict(optimizers = ['sgd','adam'], epochs = 3, batch_size = [5, 10, 20], activation = ['relu', \"tanh\"])\n",
    "# random_search = RandomizedSearchCV(model, param_distributions=params, cv=3)\n",
    "# random_search_result = random_search.fit(X,y)\n",
    "# print(\"Best: %f using %s\".format(random_search_result.best_score_,random_search_result.best_params))\n",
    "\n",
    "# # Cross Validation \n",
    "\n",
    "# def create_model2(nl = 1, nn=256):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(5, input_shape=(4,), activation=\"relu\"))\n",
    "#     for in range(nl):\n",
    "#         model.add(Dense(nn,activation = \"relu\"))\n",
    "#     model.add(Dense(1, activation = \"sigmoid\"))\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "#     return model\n",
    "# params = dict(nl = [1, 2, 9], nn = [128,256,1000])\n",
    "# model = KerasClassifier(build_fn=create_model, epochs= 6, batch_size = 16)\n",
    "# kfolds = cross_val_score(model, X, y, cv = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25f4f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Another Example of Checking Hyperparameters\n",
    "# def create_model(learning_rate, activation):\n",
    "#     opt = Adam(lr = learning_rate)\n",
    "#   \tmodel = Sequential()\n",
    "#   \tmodel.add(Dense(128, input_shape = (30,), activation = activation))\n",
    "#   \tmodel.add(Dense(256, activation = activation))\n",
    "#   \tmodel.add(Dense(1, activation = 'sigmoid'))\n",
    "#   \tmodel.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = ['accuracy'])\n",
    "#   \treturn model\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# model = KerasClassifier(build_fn = create_model)\n",
    "# params = {'activation': [\"relu\", \"tanh\"], 'batch_size': [32, 128, 256], \n",
    "#           'epochs': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "# random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
    "# show_results()\n",
    "\n",
    "# # Part 2\n",
    "# model = KerasClassifier(build_fn = create_model(learning_rate = 0.001, activation = \"relu\"), epochs = 50, \n",
    "#              batch_size = 128, verbose = 0)\n",
    "# kfolds = cross_val_score(model, X, y, cv = 3)\n",
    "# print('The mean accuracy was:', kfolds.mean())\n",
    "# print('With a standard deviation of:', kfolds.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfa55df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Accessing to layers\n",
    "# first_layer = model.layers[0]\n",
    "# print(first_layer.input)\n",
    "# print(first_layer.output)\n",
    "# print(first_layer.weights)\n",
    "\n",
    "# import tensorflow.keras.backend as K\n",
    "# inp = first_layer.input\n",
    "# out = first_layer.output\n",
    "# in_to_out = K.function([inp],[out])\n",
    "# print(in_to_out([X_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce86d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Building an Auto-Encoder\n",
    "# autoencoder = Sequential()\n",
    "# autoencoder.add(Dense(100, input_shape = (5,), activation = \"relu\"))\n",
    "# autoencoder.add(Dense(100, activation = \"sigmoid\"))\n",
    "# model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")\n",
    "\n",
    "# encoder = Sequential()\n",
    "# encoder.add(autoencoder.layers[0])\n",
    "# encoder.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e969d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of an Autoencoder\n",
    "\n",
    "# # Start with a sequential model\n",
    "# autoencoder = Sequential()\n",
    "\n",
    "# # Add a dense layer with input the original image pixels and neurons the encoded representation\n",
    "# autoencoder.add(Dense(32, input_shape=(28*28, ), activation=\"relu\"))\n",
    "\n",
    "# # Add an output layer with as many neurons as the orginal image pixels\n",
    "# autoencoder.add(Dense(28*28, activation = \"sigmoid\"))\n",
    "\n",
    "# # Compile your model with adadelta\n",
    "# autoencoder.compile(optimizer = \"adadelta\", loss = \"binary_crossentropy\")\n",
    "\n",
    "# # Summarize your model structure\n",
    "# autoencoder.summary()\n",
    "\n",
    "# # Build your encoder by using the first layer of your autoencoder\n",
    "# encoder = Sequential()\n",
    "# encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# # Encode the noisy images and show the encodings for your favorite number [0-9]\n",
    "# encodings = encoder.predict(X_test_noise)\n",
    "# show_encodings(encodings, number = 1)\n",
    "\n",
    "# # Predict on the noisy images with your autoencoder\n",
    "# decoded_imgs = autoencoder.predict(X_test_noise)\n",
    "\n",
    "# # Plot noisy vs decoded images\n",
    "# compare_plot(X_test_noise, decoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49b6e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dealing with Images, CNN models\n",
    "\n",
    "# from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(filters = 32, kernel_size = 3, input_shape = (28,28,1), activation = 'relu'))\n",
    "# model.add(Conv2D(filters = 8, kernel_size = 3, activation = 'relu'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(3, activation = \"softmax\"))\n",
    "\n",
    "# # Preprocessing images for ResNet50\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "# from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50, decode_predictions\n",
    "# img = image.load_img(img_path, target_size = (224,224))\n",
    "# img_array = image.img_to_array(img)\n",
    "# img_expanded = image.expand_dims(img_array, axis = 0)\n",
    "# img_pre = preprocess_input(img_expanded)\n",
    "# model = ResNet50(weights = 'imagenet')\n",
    "# preds = model.predict(img_pre)\n",
    "# print(\"predicted:\", decode_predictions(preds, top=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d99ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet Example\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "# from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50, decode_predictions\n",
    "# img = image.load_img(img_path, target_size=(224, 224))\n",
    "# img_array = image.img_to_array(img)\n",
    "# img_expanded = np.expand_dims(img_array, axis = 0)\n",
    "# img_ready = preprocess_input(img_expanded)\n",
    "# model = ResNet50(weights=\"imagenet\")\n",
    "# preds = model.predict(img_ready)\n",
    "# print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "281c6c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi this is', 'this is a', 'is a small', 'a small sentense']\n",
      "{1: 'is', 2: 'a', 3: 'this', 4: 'small', 5: 'hi', 6: 'sentense'}\n"
     ]
    }
   ],
   "source": [
    "# # LSTM model\n",
    "# text = 'Hi this is a small sentense'\n",
    "# seq_len = 3\n",
    "# words = text.split()\n",
    "# lines = []\n",
    "# for i in range(seq_len, len(words)+1):\n",
    "#     line = \" \".join(words[i-seq_len:i])\n",
    "#     lines.append(line)\n",
    "# print(lines)\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(lines)\n",
    "# sequences = tokenizer.texts_to_sequences(lines)\n",
    "# print(tokenizer.index_word)\n",
    "\n",
    "# from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "# model = Sequential()\n",
    "# vocab_size = len(tokenizer.index_word) + 1\n",
    "# model.add(Embedding(input_dim = vocab_size, output_dim = 8, input_length = 2))\n",
    "# model.add(LSTM(8))\n",
    "# model.add(Dense(8, activation = \"relu\"))\n",
    "# model.add(Dense(vocab_size, activation = \"softmax\"))\n",
    "# def predict_text(test_text, model = model):\n",
    "#     if len(test_text.split()) != 3:\n",
    "#     print('Text input should be 3 words!')\n",
    "#     return False\n",
    "  \n",
    "#     # Turn the test_text into a sequence of numbers\n",
    "#     test_seq = tokenizer.texts_to_sequences([test_text])\n",
    "#     test_seq = np.array(test_seq)\n",
    "  \n",
    "#      # Use the model passed as a parameter to predict the next word\n",
    "#     pred = model.predict(test_seq).argmax(axis = 1)[0]\n",
    "  \n",
    "#     # Return the word that maps to the prediction\n",
    "#     return tokenizer.index_word[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04954ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
